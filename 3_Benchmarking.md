# ðŸ“Š Benchmarking

Resources and tools for evaluating AI performance on edge devices. Standard benchmarking approaches are essential to achieve a comparable study with the scientific community. We recommend reading into, **The International Open Benchmark Council (BenchCouncil)**, a international research institution dedicated to the interdisciplinary discipline of Evaluation Science and Engineering, known asÂ **Evaluatology**.

- **[Edge AIBench V3.0](https://www.benchcouncil.org/aibench/edge-aibench/index.html)** â€” Benchmark suite for AI inference on edge platforms.
- **[AIoTBench](https://www.benchcouncil.org/aibench/aiotbench/index.html)** â€” Benchmarks targeting AI + IoT workloads
- **[EEMBC MLMarkÂ®](https://www.eembc.org/mlmark/)** - 
- **[MLMark](https://www.eembc.org/mlmark/)** â€” Machine learning performance metrics for embedded platforms.
- **[MLPerf Tiny Inference](https://mlcommons.org/en/inference-tiny-10/)** â€” Industry-standard benchmarks for ultra-low-power devices.
- **[EDLAB](https://github.com/HPInc/EDLAB)** â€” Open-source deep learning accelerator benchmark framework.
