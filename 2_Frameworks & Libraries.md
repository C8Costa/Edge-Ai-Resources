# ðŸ§  Frameworks & Libraries

A collection of tools and runtimes for deploying AI models on edge devices.

###### Crossâ€‘Platform Runtimes 

Generalâ€‘purpose, lightweight inference engines you can embed into mobile apps or edge devices regardless of vendor:
- **[TensorFlow Lite](https://www.tensorflow.org/lite)** - Lightweight version of TensorFlow for mobile and embedded devices.
- **[ExecuTorch](https://pytorch.org/mobile/home/)** - PyTorchâ€™s solution to training and inference on the Edge.
- **[ONNX Runtime](https://onnxruntime.ai/)** - Cross-platform, high-performance scoring engine for ML models.
- **[Apache TVM](https://tvm.apache.org/)** - A open-source machine learning compiler framework for CPUs, GPUs, and machine learning accelerators.
- **[Edge Impulse - Edge Ai Platform](https://edgeimpulse.com/product?_gl=1*1sp61fb*_up*MQ..*_ga*MTUzMzA1Njk2My4xNzUzNzIyMDAx*_ga_1QH4BGX503*czE3NTM3MjIwMDEkbzEkZzEkdDE3NTM3MjIxNjIkajI4JGwwJGgxNjU1NjkzOTgz)** - Let's users collect sensor data, build and version ML pipelines, shrink models with its **EONâ€¯Compiler**, and generate turnkey firmware for deployment on a variety of edge devices.

###### Acceleratorâ€‘Specific Inference Toolkits

Optimized stacks from silicone manufacturers (GPU, FPGA, CPU)  :

- **[TensorRT](https://developer.nvidia.com/tensorrt)** NVIDIAâ€™s GPUâ€‘focused, mixedâ€‘precision inference library.
- **[AMD Vitis AI](https://xilinx.github.io/Vitis-AI/3.5/html/index.html)** - Deepâ€‘learning dev stack targeting AMDâ€‘Xilinx FPGAs and adaptive SoCs.
- **[OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)** - Intelâ€™s toolkit for optimizing + deploying inference on CPUs, iGPUs, VPUs and FPGAs.
- **[STM32Cube.AI](https://www.st.com/en/embedded-software/x-cube-ai.html)** - STâ€™s code generator to run neural nets on STM32 microcontrollers.
- **[e-AI Development Environment](https://www.renesas.com/us/en/e-ai-development-environment-microcontrollers#overview)** - Tools for Running Learned AI on Renesas MCUs and MPUs.
- **[nncase](https://github.com/kendryte/nncase)** - Open deep learning compiler stack for Kendryte AI accelerators
